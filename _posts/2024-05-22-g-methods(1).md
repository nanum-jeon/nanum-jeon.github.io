---
title: "Replications for introduction to g-methods: g-formula (part 1)"
output:
  md_document:
    variant: gfm+footnotes
    preserve_yaml: TRUE
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../_posts") })
date: 2024-05-22
permalink: /posts/2024/05/g-methods1
excerpt: In this tutorial, we will focus on replicating the results using **g-formula** to estimate the average causal effects of always taking treatment and compared to never-taking treatment. 
always_allow_html: true
toc: true
header:
  og_image: "posts/g-methods/time_chart.png"
tags:
  - tutorial
  - causality
---
This tutorial series aims to replicate g-methods explained in [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074945/) by Naimi, A. I., Cole, S. R., and Kennedy, E. H (2017)[^1] using **R**. Originally, the paper used SAS to demonstrate g-methods. 

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<details style="padding-bottom: 20px;">
<summary><strong>Reminder: Our Settings</strong></summary>

<div style="padding-top: 20px;">
  
The empirical setting is to treat HIV with a therapy regimen ($A$) in two time periods ($t = 0, t = 1$). Additionally, we measure the time-varying confounder, HIV viral load ($Z$) at times $t = 0$ and $t = 1$. Note that this time-varying confounder is measured prior to the treatment administered at each time period. Also, we assume $Z$ at time 0 is 1 (high, bad health condition) for all subjects. Our outcome is the CD4 count (cells/mm$^3$) observed at $t = 2$. 

Thus, we have:  

<img src="/images/posts/g-methods/time_chart.png" style="display: block; margin: auto;" />

Under the identifying assumptions described in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6074945/">the paper</a>, we will estimate the average casual effect of always taking treatment ($a_0 = 1, a_1 = 1$), compared to never taking treatment ($a_0 = 0, a_1 = 0$) in both time periods. For notation, we are using subscripts to indicate time periods.
</div>

</details>


In this tutorial, we will focus on replicating the results using **g-formula** to esitmate the average causal effects of always taking treatment and compared to never-taking treatment. From our knowledge on the data generating process, we know this average causal effect to be $$50$$. In our tutorial, we will pay more attention to computation rather than proofs to perform replciations.  


# G-formula with computation 

The g-formula can be used to estimate the average potential outcome level that would be observed in the population (that is, marginal outcome[^2]) under a hypotetical treatment intervention. We esitmate the marginal mean of Y that would be observed if $$A_0$$ $$A_1$$ were set to some values $$a_0$$ and $$a_1$$ hypothetically using the estimator on the right hand side of the quesiton below.  

$$ \begin{align*} 
E(Y^{a_0, a_1}) =  \textstyle\sum_{a_1, z_1, a_0} \,\,&E(Y | A_1 = a_1, Z_1 = z_1, A_0 = a_0)\\\
&P(A_1 = a_1 | Z_1 = z_1, A_0 = a_0)\\\
&P(Z_1 = z_1 | A_0 = a_0)\\\
&P(A_0 = a_0)
\end{align*} 
$$ 

Because we hypothetically intervene on treatment, we are able to simplify our estimator. This simplification is possible because, as we intervene on treatment, the probability density for treatment becomes irrelevant; it is already determined at the point of our intervention.

$$
\begin{align*}
E(Y^{a_0, a_1}) =  \textstyle\sum_{z_1} \,\,&E(Y | A_1 = a_1, Z_1 = z_1, A_0 = a_0)\\\ 
&P(Z_1 = z_1 | A_0 = a_0) 
\end{align*}
$$

Note that the right side of the equation is essentially equivalent to computing a weighted average of $$Y$$ marginalized over $$Z_1$$ given $$A_0$$.  

In our scenario, we compute two expected values to estimate our estimand: the average causal effects of always being treated compared to never being treated. Thus, we are interested in computing two expected values for the always-treated intervention $$E[Y^{a_0 = 1, a_1 = 1}]$$, and for the never-treated intervention, $$E[Y^{a_0 = 0, a_1 = 0}]$$. Finally, our estimand of interest ($$\hat{\tau}$$) is calucated as: 

$$\hat{\tau} = E[Y^{a_0 = 1, a_1 = 1}] - E[Y^{a_0 = 0, a_1 = 0}]$$

R-codes below demonstrate how we compute these expected values step-by-step. 

## Always treated internvetion 

```r
# E[Y^{1, 1}]

# The total number subjects in the A0 = 1 stratum 
a0_1_n <- data %>% 
  filter(a0 == 1) %>% 
  summarise(a0_1_n = sum(n)) %>% 
  pull()

#  Weights for Z1 = 0 and Z1 = 1 among those with A0 = 1 
weight <- data %>% 
  filter(a0 == 1) %>% 
  group_by(z1) %>% 
  summarise(weight = sum(n) / a0_1_n) %>% 
  select(z1, weight)

# Compute expectation’s value for always treated
expected_value_always_treated <- data %>% 
  filter(a0 == 1 & a1 == 1) %>% 
  left_join(weight, by = "z1") %>% 
  mutate(weighted_y = y * weight) %>% 
  group_by(a0, a1) %>% 
  summarise(weighted_sum = sum(weighted_y)) 

expected_value_always_treated
```

## Never treated internvetion 
```r
# E[Y^{0, 0}]
# Never treated 

# The total number subjects in the A0 = 0 stratum 
a0_0_n <- data %>% 
  filter(a0 == 0) %>% 
  summarise(a0_0_n = sum(n)) %>% 
  pull()

# Weights for Z1 = 0 and Z1 = 1 among those with A0 = 0 
weight <- data %>% 
  filter(a0 == 0) %>% 
  group_by(z1) %>% 
  summarise(weight = sum(n) / a0_0_n) %>% 
  select(z1, weight)

# Compute expectation’s value for those never treated 
expected_value_never_treated <- data %>% 
  filter(a0 == 0 & a1 == 0) %>% 
  left_join(weight, by = "z1") %>% 
  mutate(weighted_y = y * weight) %>% 
  group_by(a0, a1) %>% 
  summarise(weighted_sum = sum(weighted_y)) 

expected_value_never_treated
```

Finally, we report the results in Table below. We find that the average causal effect ($$\hat{\tau}$$) is $$50 = 150-500$$ as shown by the authors. 

| a0| a1| weighted_sum|
|--:|--:|------------:|
|  1|  1|     150.0621|
|  0|  0|     100.0370|


This approach to computing the value of the **g-formula** is called *nonparametric maximum likelihood estimation*. 

  
# G-formula with parametric models  

However, we often use simulation from parametric regression models to construct a  **g-formula** estimator. This approach is typically the most realistic when dealing with many covariates in our data. Now, we will discuss how to use parametric regression models to apply the g-formula. Although the web supplement includes the procedures, the results from the parametric g-formula are not included in the main article.

As the first step, we fit a model for each compoment of the likelihood in our estimator following the order of the events in time. As the below digram shows, $$Z_1$$ is affected by $$A_0$$, and $A_1$ is administered depending on the level of $Z_1$. Finally, $Y$ is affected by all treatments $$A_0, A_1$$ and time varying confounder $Z_1$ (recall $Z_0$ is fixed at 1 for all observations, and thus gives us no additional information). 

<img src="/images/posts/g-methods/time_chart.png" style="display: block; margin: auto;" />

Given this relationship of the variables, we fit the models in R. 

## Step 1: Fit models 
```r
# Step 1: Fit a model for each component of the likelihood 
# Note we disregard the number of subjects for each stratum and weight them equally 

z1_model  <- glm(z1 ~ a0, family = binomial())
a1_model <- glm(a1 ~ z1, family = binomial())
y_model <- lm(y ~ a1 + z1 + a0 + a1*a0, data = data)
```

Note that we are using logistic models for $Z_1$ and $A_1$ since they are binary variables, and a linear model for $Y$. All these models include parametric assumptions about the relationships between the variables, which may bias our estimates if these assumptions do not hold in our data-generating processes.

## Step 2: Generate simulated values for $A$, $Z$ and $Y$ using models 

We now generate simulated values for each variable $A$, $Z$, and $Y$ using our models. For the baseline variable $A_0$, we randomly sample it with replacement from the data, with $n = 1,000,000$. Then, we feed these simulated values into the subsequent models that take into account the sequence of events. Specifically, to simulate $Z_1$, we use the $Z_1$-model to predict the probabilities of having $Z_1 = 1$ given the simulated values of $A_0$ from the previous step, and use \texttt{rbinom} to generate simulated binary values for $Z_1$. Next, we feed these simulated values $(A_0, Z_1)$ into the $A_1$-model to generate binary simulated values for $A_1$, following a similar procedure. Finally, using the simulated values of $A_0, Z_1$ and $A_1$, we simulate $Y$ using the $Y$-model.



```r
# Step 2: Generate simulated values 
set.seed(78419)

sim_n = 1000000

a0_sim <- data.frame(a0 = sample(a0, size = sim_n, replace = TRUE))


# simulation for z1 using the z1 model that is fitted with original data 
# n = number of simulation
# prob = predicted probability from the z1 model 

z1_sim <- rbinom(n = sim_n, 
                 size = 1, 
                 prob = predict(z1_model, 
                                newdata = a0_sim, 
                                type = "response"))

# simulation for a1 using the a1 model that is fitted with original data 
# n = number of simulation
# prob = predicted probability from the a1 model 

a1_sim <- rbinom(n = sim_n, 
                 size = 1, 
                 prob = predict(a1_model, 
                                newdata = data.frame(z1 = z1_sim), 
                                type = "response"))

# simulation for y using the y model that is fitted with original data 
# n = number of simulation
# prob = predicted probability from the y model 

y_sim <- predict(y_model, 
                   newdata = data.frame(a0 = a0_sim, 
                                        z1 = z1_sim, 
                                        a1 = a1_sim), 
                   type = "response")

```


## Step 3: Intervene on treatment to compute g-formula using simulation 

To estimate the average causal effects of being always treated compared to never being treated, we now hypothetically intervene on treatment according to each scenario using the simulated data as follows. Then, we compute the differences in means for each scenario (always treated - never treated). 

```r
# Step 3: Intervene on treatment to compute g-formula using simulation 
data_sim <- data.frame(a0 = a0_sim, 
                       z1 = z1_sim,
                       a1 = a1_sim, 
                       y = y_sim)

# always treated
always_treated <- predict(y_model, 
                   newdata = data_sim %>% 
                     mutate(a0 = 1, 
                            a1 = 1), 
                   type = "response")

# never treated 
never_treated <- predict(y_model, 
                         newdata = data_sim %>% 
                           mutate(a0 = 0, 
                                  a1 = 0), 
                   type = "response")

# Compute average potential outcome under each intervention scenario
potential_y <- data.frame(
  a0 = c(1, 0),
  a1 = c(1, 0), 
  potential_outcome = c(mean(always_treated), mean(never_treated)))
```


We now report the results from the parametric g-formula below. As we see, $\hat{\tau}_{\text{parametric}}$ is equal to $43.037 = 146.4731 - 103.4361$. This estimate is lower than the true causal effect that we already know from the data generation process, indicating our parametric models may indeed be misspecified, leading to biased estimates.



| a0| a1| potential_outcome|
|--:|--:|-----------------:|
|  1|  1|          146.4731|
|  0|  0|          103.4361|


Using the parametric models, however, we can more easily estimate potential outcomes under other intervention scenarios, as shown below. 

```r
# other intervention scenarios 

# natural course 
natural_course <- predict(y_model, 
                   newdata = data_sim, 
                   type = "response")

# treated only at time 0 
treated_time0 <- predict(y_model, 
                   newdata = data_sim %>% 
                     mutate(a0 = 1,
                            a1 = 0), 
                   type = "response")

# treated only at time 1 
treated_time1 <- predict(y_model, 
                   newdata = data_sim %>% 
                     mutate(a0 = 0,
                            a1 = 1), 
                   type = "response")

potential_y <- data.frame(
  a0 = c(1, 0, "natural course"),
  a1 = c(0, 1, "natural course"), 
  potential_outcome = c(mean(treated_time0), mean(treated_time1), mean(natural_course)))
```

The results from other interventions are reported below. 


|a0             |a1             | potential_outcome|
|:--------------|:--------------|-----------------:|
|1              |0              |          121.4661|
|0              |1              |          128.4396|
|natural course |natural course |          124.9453|



# Switch to Machine Learning Models

Parametric models used in this paper, such as generalized linear models, assume a linear relationship between the covariates ($A, Z$) and the response variable ($Y$) either directly through the parameters or indirectly through a link function. While effective under certain conditions, these models are limited by their assumption of linearity, and may lead to biased estimates as shown in our example. 

Machine learning models, which can accommodate more complex, non-linear relationships, offer a flexible alternative. However, like all models, they are not immune to misspecification issues such as overfitting or underfitting, regardless of their complexity. For the next section of this tutorial, we will explore various machine learning techniques that can handle non-linearity more robustly.





[^1]: Naimi, A. I., Cole, S. R., & Kennedy, E. H. (2017). An introduction to g methods. International journal of epidemiology, 46(2), 756-762.
[^2]: My two cents: the term <em>marginal</em> in plain English usually means <ins>of less importance</ins>. But in our context, the term <em>marginal</em> is used to indicate <ins>population</ins> level outcome in contrast to <ins>individual</ins> level outcome. In some sense, it might make sense, because it would be the best if we are able to estimate <ins>individual</ins> treatment effects. As it is often not very feasible, we instead rely on <ins>population</ins> level estimates, which could be <ins>less important</ins> in some sense. However, considering that the population encompasses all individuals, it can sometimes be puzzling why we refer to it as <ins>marginal,<ins> as it is certainly bigger! 


